model_name,organization,release_date,source_url,report_path,notes,ARC-Challenge,ARC-Challenge__setting,BIG-Bench Hard,BIG-Bench Hard__setting,CSQA,CSQA__setting,DROP,DROP__setting,GPQA,GPQA__setting,GPQA (Diamond),GPQA (Diamond)__setting,GSM8K,GSM8K__setting,HellaSwag,HellaSwag__setting,HumanEval,HumanEval__setting,HumanEval+,HumanEval+__setting,MATH,MATH__setting,MBPP,MBPP__setting,MBPP+,MBPP+__setting,MGSM,MGSM__setting,MMLU,MMLU__setting,MMLU-Pro,MMLU-Pro__setting,MMLU-redux,MMLU-redux__setting,MMLU-stem,MMLU-stem__setting,Multi-Exam,Multi-Exam__setting,Multi-Mathematics,Multi-Mathematics__setting,Multi-Translation,Multi-Translation__setting,Multi-Understanding,Multi-Understanding__setting,MultiPL-E,MultiPL-E__setting,Natural Questions,Natural Questions__setting,TheoremQA,TheoremQA__setting,TriviaQA,TriviaQA__setting,TruthfulQA,TruthfulQA__setting,WinoGrande,WinoGrande__setting,C-Eval,C-Eval__setting,CMMLU,CMMLU__setting,IFEval,IFEval__setting
StableLM-Alpha 3B,Stability AI,2023-04-20,https://github.com/Stability-AI/StableLM,open reports/combined_report.md,Open weights; 3B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StableLM-Alpha 7B,Stability AI,2023-04-20,https://github.com/Stability-AI/StableLM,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MPT-7B,MosaicML,2023-05-05,https://www.businesswire.com/news/home/20230622195151/en/MosaicML-Releases-Open-Source-MPT-30B-LLMs-Trained-on-H100s-to-Power-Generative-AI-Applications,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RedPajama-INCITE 3B,Together,2023-05-05,https://www.together.ai/blog/redpajama-models-v1,open reports/combined_report.md,Open weights; 3B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Falcon 40B,Technology Innovation Institute,2023-05-25,https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model,open reports/combined_report.md,Open weights; 40B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RedPajama-INCITE 7B,Together,2023-06-06,https://www.together.ai/blog/redpajama-7b,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MPT-30B,MosaicML,2023-06-22,https://www.businesswire.com/news/home/20230622195151/en/MosaicML-Releases-Open-Source-MPT-30B-LLMs-Trained-on-H100s-to-Power-Generative-AI-Applications,open reports/combined_report.md,Open weights; 30B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 2 13B,Meta,2023-07-18,https://about.fb.com/news/2023/07/llama-2/,open reports/combined_report.md,Open weights; 13B parameters. Benchmarks from Meta Llama 3 model card (base pretrained models table).,67.6,25-shot,47.0,3-shot CoT,,,49.8,3-shot (F1),,,,,,,,,,,,,,,,,,,,,53.8,5-shot,,,,,,,,,,,,,,,,,,,,,79.6,5-shot (Wiki),,,75.4,5-shot,,,,,,
Llama 2 70B,Meta,2023-07-18,https://about.fb.com/news/2023/07/llama-2/,open reports/combined_report.md,Open weights; 70B parameters. Benchmarks from Meta Llama 3 model card (base pretrained models table).,85.3,25-shot,65.7,3-shot CoT,,,70.2,3-shot (F1),,,,,56.8 (accuracy),8-shot,,,29.9 (pass@1),0-shot,,,,,,,,,,,69.7,5-shot,,,,,,,,,,,,,,,,,33.0 (accuracy),1-shot,,,87.5,5-shot (Wiki),,,81.8,5-shot,,,,,,
Llama 2 7B,Meta,2023-07-18,https://about.fb.com/news/2023/07/llama-2/,open reports/combined_report.md,Open weights; 7B parameters. Benchmarks from Meta Llama 3 model card (base pretrained models table).,53.7,25-shot,38.1,3-shot CoT,,,37.9,3-shot (F1),,,,,,,,,,,,,,,,,,,,,45.7,5-shot,,,,,,,,,,,,,,,,,,,,,72.1,5-shot (Wiki),,,73.3,5-shot,,,,,,
Qwen 7B,Alibaba,2023-08-03,https://qwenlm.github.io/blog/qwen/,open reports/combined_report.md,Open weights; 7B parameters.,,,45.0 (reported score),3-shot,,,,,,,,,51.7 (reported score),8-shot,,,29.9 (reported score),0-shot,,,11.6 (reported score),4-shot,31.6 (reported score),3-shot,,,,,58.2 (reported score),5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code Llama 13B,Meta,2023-08-24,https://about.fb.com/news/2023/08/code-llama-ai-for-coding/,open reports/combined_report.md,"Open weights; 13B parameters. Benchmarks from Code Llama paper (arXiv 2308.12950v3), Table 2.",,,,,,,,,,,,,,,,,36.0,"0-shot, pass@1 (greedy)",,,,,47.0,"3-shot, pass@1 (greedy)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code Llama 34B,Meta,2023-08-24,https://about.fb.com/news/2023/08/code-llama-ai-for-coding/,open reports/combined_report.md,"Open weights; 34B parameters. Benchmarks from Code Llama paper (arXiv 2308.12950v3), Table 2.",,,,,,,,,,,,,,,,,48.8,"0-shot, pass@1 (greedy)",,,,,55.0,"3-shot, pass@1 (greedy)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code Llama 7B,Meta,2023-08-24,https://about.fb.com/news/2023/08/code-llama-ai-for-coding/,open reports/combined_report.md,"Open weights; 7B parameters. Benchmarks from Code Llama paper (arXiv 2308.12950v3), Table 2.",,,,,,,,,,,,,,,,,33.5,"0-shot, pass@1 (greedy)",,,,,41.4,"3-shot, pass@1 (greedy)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Baichuan2 13B,Baichuan,2023-09-06,https://github.com/baichuan-inc/Baichuan-7B,reports/combined_report.md,Open weights; 13B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Baichuan2 7B,Baichuan,2023-09-06,https://github.com/baichuan-inc/Baichuan-7B,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen 14B,Alibaba,2023-09-25,https://qwenlm.github.io/blog/qwen/,open reports/combined_report.md,Open weights; 14B parameters.,,,53.4 (reported score),3-shot,,,,,,,,,61.3 (reported score),8-shot,,,32.3 (reported score),0-shot,,,24.8 (reported score),4-shot,40.8 (reported score),3-shot,,,,,66.3 (reported score),5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mistral 7B,Mistral AI,2023-09-27,https://mistral.ai/news/announcing-mistral-7b,open reports/combined_report.md,Open weights; 7B parameters. Benchmarks from Mixtral of Experts paper (arXiv 2401.04088) Table 2; MBPP uses hand-verified subset and TriviaQA has no Wikipedia context (per paper).,54.9,0-shot,,,,,,,,,,,50.0,8-shot (maj@8),81.0,0-shot,26.2,0-shot,,,12.7,4-shot (maj@4),50.2,3-shot (hand-verified subset),,,,,62.5,5-shot,,,,,,,,,,,,,,,,,23.2,5-shot,,,62.5,5-shot (no Wikipedia context),,,74.2,0-shot,,,,,,
Yi 34B,01.AI,2023-11-02,https://github.com/01-ai/Yi,reports/combined_report.md,Open weights; 34B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yi 6B,01.AI,2023-11-02,https://github.com/01-ai/Yi,reports/combined_report.md,Open weights; 6B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen 1.8B,Alibaba,2023-11-30,https://qwenlm.github.io/blog/qwen/,open reports/combined_report.md,Open weights; 1.8B parameters.,,,,,,,,,,,,,32.3 (reported score),8-shot,,,15.2 (reported score),0-shot,,,,,,,,,,,45.3 (reported score),5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen 72B,Alibaba,2023-11-30,https://qwenlm.github.io/blog/qwen/,open reports/combined_report.md,Open weights; 72B parameters.,,,67.7 (reported score),3-shot,,,,,,,,,78.9 (reported score),8-shot,,,35.4 (reported score),0-shot,,,35.2 (reported score),4-shot,52.2 (reported score),3-shot,,,,,77.4 (reported score),5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixtral 8x7B,Mistral AI,2023-12-11,https://legal.mistral.ai/ai-governance/models/mixtral-8-7b,open reports/combined_report.md,Open weights; MoE 46.7B total parameters. Benchmarks from Mixtral of Experts paper (arXiv 2401.04088) Table 2; MBPP uses hand-verified subset and TriviaQA has no Wikipedia context (per paper).,59.7,0-shot,,,,,,,,,,,74.4,8-shot (maj@8),84.4,0-shot,40.2,0-shot,,,28.4,4-shot (maj@4),60.7,3-shot (hand-verified subset),,,,,70.6,5-shot,,,,,,,,,,,,,,,,,30.6,5-shot,,,71.5,5-shot (no Wikipedia context),,,77.2,0-shot,,,,,,
Phi-2 2.7B,Microsoft,2023-12-12,https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/,open reports/combined_report.md,Open weights; 2.7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternLM2 20B,Shanghai AI Laboratory,2024-01-17,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 20B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternLM2 7B,Shanghai AI Laboratory,2024-01-17,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternLM2 1.8B,Shanghai AI Laboratory,2024-01-31,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 1.8B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLMo 1B,Ai2,2024-02-01,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 1B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLMo 7B,Ai2,2024-02-01,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 1.8B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 1.8B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 110B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 110B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 14B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 14B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 32B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 32B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 4B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 4B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 72B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 72B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen1.5 7B,Alibaba,2024-02-04,https://qwenlm.github.io/blog/qwen1.5/,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gemma 2B,Google,2024-02-21,https://ai.google.dev/gemma/docs/releases,open reports/combined_report.md,Open weights; 2B parameters. Benchmarks from Gemma model card (PT results table).,42.1,ARC-c,,,,,,,,,,,17.7,maj@1,71.4,0-shot,22.0,pass@1,,,11.8,4-shot,29.2,3-shot,,,,,42.3,"5-shot, top-1",,,,,,,,,,,,,,,,,12.5,5-shot,,,53.2,5-shot,,,65.4,partial score,,,,,,
Gemma 7B,Google,2024-02-21,https://ai.google.dev/gemma/docs/releases,open reports/combined_report.md,Open weights; 7B parameters. Benchmarks from Gemma model card (PT results table).,53.2,ARC-c,,,,,,,,,,,46.4,maj@1,81.2,0-shot,32.3,pass@1,,,24.3,4-shot,44.4,3-shot,,,,,64.3,"5-shot, top-1",,,,,,,,,,,,,,,,,23.0,5-shot,,,63.4,5-shot,,,72.3,partial score,,,,,,
StarCoder2 15B,BigCode,2024-02-28,https://huggingface.co/blog/starcoder2,reports/combined_report.md,Open weights; 15B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StarCoder2 3B,BigCode,2024-02-28,https://huggingface.co/blog/starcoder2,reports/combined_report.md,Open weights; 3B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StarCoder2 7B,BigCode,2024-02-28,https://huggingface.co/blog/starcoder2,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DBRX 132B,Databricks,2024-03-27,https://www.databricks.com/company/newsroom/press-releases/databricks-launches-dbrx-new-standard-efficient-open-source-models,open reports/combined_report.md,Open weights; MoE 132B total parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stable LM 2 1.6B,Stability AI,2024-04-08,https://stability.ai/news/introducing-stable-lm-2-12b,open reports/combined_report.md,Open weights; 1.6B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stable LM 2 12B,Stability AI,2024-04-08,https://stability.ai/news/introducing-stable-lm-2-12b,open reports/combined_report.md,Open weights; 12B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeGemma 2B,Google,2024-04-09,https://developers.googleblog.com/en/gemma-family-expands-with-models-tailored-for-developers-and-researchers/,reports/combined_report.md,Open weights; 2B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeGemma 7B,Google,2024-04-09,https://developers.googleblog.com/en/gemma-family-expands-with-models-tailored-for-developers-and-researchers/,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixtral 8x22B,Mistral AI,2024-04-17,https://mistral.ai/news/mixtral-8x22b/,open reports/combined_report.md,"Open weights; MoE 141B total parameters. Benchmarks from Mistral AI Mixtral 8x22B blog; values are for instructed version (GSM8K maj@8, Math maj@4).",,,,,,,,,,,,,90.8,"maj@8 (8-shot, instruct)",,,,,,,44.6,maj@4 (instruct),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3 70B,Meta,2024-04-18,https://about.fb.com/ltam/news/2024/04/presentamos-meta-llama-3-el-modelo-de-lenguaje-de-gran-tamano-mas-potente-hasta-la-fecha/,open reports/combined_report.md,Open weights; 70B parameters. Benchmarks from Meta Llama 3 model card (base pretrained models table). Benchmarks from Llama 3 technical report (arXiv 2407.21783) base pretrained table.,64.6,25-shot,82.6,"3-shot, CoT",,,79.7,"3-shot, F1",41.7,,,,89.0,,85.3,,81.7,,,,66.6,,84.6,,,,,,79.5,5-shot,56.8,,71.0,,,,,,,,,,,,,,,,,,89.7,5-shot,,,83.1,5-shot,,,,,,
Llama 3 8B,Meta,2024-04-18,https://about.fb.com/ltam/news/2024/04/presentamos-meta-llama-3-el-modelo-de-lenguaje-de-gran-tamano-mas-potente-hasta-la-fecha/,open reports/combined_report.md,Open weights; 8B parameters. Benchmarks from Meta Llama 3 model card (base pretrained models table). Benchmarks from Llama 3 technical report (arXiv 2407.21783) base pretrained table.,63.7,25-shot,74.6,"3-shot, CoT",,,58.4,"3-shot, F1",34.2,,,,78.4,,80.5,,62.2,,,,50.4,,78.6,,,,,,68.4,5-shot,45.7,,64.3,,,,,,,,,,,,,,,,,,78.5,5-shot,,,76.1,5-shot,,,,,,
Phi-3 Mini 3.8B,Microsoft,2024-04-23,https://www.microsoft.com/en-us/research/publication/phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/,open reports/combined_report.md,Open weights; 3.8B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Falcon 2 11B,Technology Innovation Institute,2024-05-13,https://www.tii.ae/index.php/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas,open reports/combined_report.md,Open weights; 11B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yi-1.5-34B,01.AI,2024-05-13,https://github.com/01-ai/Yi,open reports/combined_report.md,Open-source release (Yi-1.5 announcement).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yi-1.5-6B,01.AI,2024-05-13,https://github.com/01-ai/Yi,open reports/combined_report.md,Open-source release (Yi-1.5 announcement).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yi-1.5-9B,01.AI,2024-05-13,https://github.com/01-ai/Yi,open reports/combined_report.md,Open-source release (Yi-1.5 announcement).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PaliGemma 3B,Google,2024-05-14,https://ai.google.dev/gemma/docs/releases,open reports/combined_report.md,Open weights; 3B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Phi-3 Medium 14B,Microsoft,2024-05-21,https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/,reports/combined_report.md,Open weights; 14B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Phi-3 Small 7B,Microsoft,2024-05-21,https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2 1.5B,Alibaba,2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; 1.5B parameters.,43.9,25-shot,37.2,3-shot,,,,,,,,,58.5,4-shot,66.6,10-shot,31.1,0-shot (EvalPlus),,,21.7,4-shot,37.4,0-shot (EvalPlus),,,,,56.5,5-shot,21.8,5-shot,,,,,,,,,,,,,,,,,15.0,5-shot,,,45.9,0-shot,66.2,5-shot,70.6,5-shot,70.3,5-shot,,
Qwen2 57B-A14B,Alibaba,2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; MoE 57B total parameters.,64.1,25-shot,67.0,3-shot,,,,,34.3,5-shot,,,80.7,4-shot,85.2,10-shot,53.0,0-shot (EvalPlus),,,43.0,4-shot,71.9,0-shot (EvalPlus),,,,,76.5,5-shot,43.0,5-shot,,,,,65.5,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,62.3,MGSM 8-shot,34.5,Flores-101 5-shot,77.0,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,49.8,0-shot,,,33.5,5-shot,,,57.7,0-shot,79.5,5-shot,87.7,5-shot,88.5,5-shot,,
Qwen2 72B,Alibaba,2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; 72B parameters.,68.9,25-shot,82.4,3-shot,,,,,37.9,5-shot,,,89.5,4-shot,87.6,10-shot,64.6,0-shot (EvalPlus),,,51.1,4-shot,76.9,0-shot (EvalPlus),,,,,84.2,5-shot,55.6,5-shot,,,,,76.6,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,76.0,MGSM 8-shot,37.8,Flores-101 5-shot,80.7,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,59.6,0-shot,,,43.1,5-shot,,,54.8,0-shot,85.1,5-shot,91.0,5-shot,90.1,5-shot,,
Qwen2 7B,Alibaba,2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; 7B parameters.,60.6,25-shot,62.6,3-shot,,,,,31.8,5-shot,,,79.9,4-shot,80.7,10-shot,51.2,0-shot (EvalPlus),,,44.2,4-shot,65.9,0-shot (EvalPlus),,,,,70.3,5-shot,40.0,5-shot,,,,,59.2,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,57.5,MGSM 8-shot,31.5,Flores-101 5-shot,72.0,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,46.3,0-shot,,,31.1,5-shot,,,54.2,0-shot,77.0,5-shot,83.2,5-shot,83.9,5-shot,,
Qwen2-1.5B,Alibaba (Qwen Team),2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2 release.,43.9,25-shot,37.2,3-shot,,,,,,,,,58.5,4-shot,66.6,10-shot,31.1,0-shot,,,21.7,4-shot,37.4,0-shot,,,,,56.5,5-shot,21.8,5-shot,,,,,,,,,,,,,,,,,15.0,5-shot,,,45.9,0-shot,66.2,5-shot,70.6,5-shot,70.3,5-shot,,
Qwen2-57B-A14B,Alibaba (Qwen Team),2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2 release.,64.1,25-shot,67.0,3-shot,,,,,34.3,5-shot,,,80.7,4-shot,85.2,10-shot,53.0,0-shot,,,43.0,4-shot,71.9,0-shot,,,,,76.5,5-shot,43.0,5-shot,,,,,65.5,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,62.3,MGSM 8-shot,34.5,Flores-101 5-shot,77.0,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,49.8,0-shot,,,33.5,5-shot,,,57.7,0-shot,79.5,5-shot,87.7,5-shot,88.5,5-shot,,
Qwen2-72B,Alibaba (Qwen Team),2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; Qianwen License per Qwen2 release.,68.9,25-shot,82.4,3-shot,,,,,37.9,5-shot,,,89.5,4-shot,87.6,10-shot,64.6,0-shot,,,51.1,4-shot,76.9,0-shot,,,,,84.2,5-shot,55.6,5-shot,,,,,76.6,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,76.0,MGSM 8-shot,37.8,Flores-101 5-shot,80.7,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,59.6,0-shot,,,43.1,5-shot,,,54.8,0-shot,85.1,5-shot,91.0,5-shot,90.1,5-shot,,
Qwen2-7B,Alibaba (Qwen Team),2024-06-07,https://qwenlm.github.io/blog/qwen2/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2 release.,60.6,25-shot,62.6,3-shot,,,,,31.8,5-shot,,,79.9,4-shot,80.7,10-shot,51.2,0-shot,,,44.2,4-shot,65.9,0-shot,,,,,70.3,5-shot,40.0,5-shot,,,,,59.2,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,57.5,MGSM 8-shot,31.5,Flores-101 5-shot,72.0,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,46.3,0-shot,,,31.1,5-shot,,,54.2,0-shot,77.0,5-shot,83.2,5-shot,83.9,5-shot,,
RecurrentGemma 9B,Google,2024-06-11,https://ai.google.dev/gemma/docs/releases,reports/combined_report.md,Open weights; 9B parameters. Benchmarks from RecurrentGemma model card (base results table).,52.0,ARC-c,,,,,,,,,,,42.6,maj@1,80.4,0-shot,31.1,pass@1,,,23.8,4-shot,42.0,3-shot,,,,,60.5,"5-shot, top-1",,,,,,,,,,,,,,,,,21.7,5-shot,,,70.5,5-shot,,,73.6,partial score,,,,,,
Gemma 2 27B,Google,2024-06-27,https://blog.google/innovation-and-ai/technology/developers-tools/google-gemma-2/,open reports/combined_report.md,Open weights; 27B parameters. Benchmarks from Gemma 2 model card (PT results table).,71.4,ARC-c 25-shot,,,,,72.2,"3-shot, F1",,,,,74.0,"5-shot, maj@1",86.4,10-shot,51.8,pass@1,,,42.3,4-shot,62.6,3-shot,,,,,75.2,"5-shot, top-1",,,,,,,,,,,,,,,,,34.5,5-shot,,,83.7,5-shot,,,83.7,partial score,,,,,,
Gemma 2 9B,Google,2024-06-27,https://blog.google/innovation-and-ai/technology/developers-tools/google-gemma-2/,open reports/combined_report.md,Open weights; 9B parameters. Benchmarks from Gemma 2 model card (PT results table).,68.4,ARC-c 25-shot,,,,,69.4,"3-shot, F1",,,,,68.6,"5-shot, maj@1",81.9,10-shot,40.2,pass@1,,,36.6,4-shot,52.4,3-shot,,,,,71.3,"5-shot, top-1",,,,,,,,,,,,,,,,,29.2,5-shot,,,76.6,5-shot,,,80.6,partial score,,,,,,
InternLM2.5 7B,Shanghai AI Laboratory,2024-07-03,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mistral NeMo 12B,Mistral AI,2024-07-18,https://mistral.ai/it/news/mistral-nemo,open reports/combined_report.md,Open weights; 12B parameters. Benchmarks from Mistral-Nemo-Base-2407 model card (official HF).,,,,,70.4,0-shot,,,,,,,,,83.5,0-shot,,,,,,,,,,,,,68.0,5-shot,,,,,,,,,,,,,,,,,31.2,5-shot,,,73.8,5-shot,50.3,0-shot,76.8,0-shot,,,,,,
Llama 3.1 70B,Meta,2024-07-24,https://about.fb.com/es/news/2024/07/presentamos-llama-3-1-nuestro-modelo-de-ia-mas-capaz-hasta-la-fecha/,open reports/combined_report.md,Open weights; 70B parameters. Benchmarks from Llama 3 technical report (arXiv 2407.21783) base pretrained table.,68.0,,83.3,,,,,,48.0,,,,91.6,,85.5,,81.7,,,,72.0,,86.6,,,,,,82.0,,61.0,,75.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.1 8B,Meta,2024-07-24,https://about.fb.com/es/news/2024/07/presentamos-llama-3-1-nuestro-modelo-de-ia-mas-capaz-hasta-la-fecha/,open reports/combined_report.md,Open weights; 8B parameters. Benchmarks from Llama 3 technical report (arXiv 2407.21783) base pretrained table.,65.7,,75.1,,,,,,36.2,,,,83.1,,82.0,,72.6,,,,54.4,,79.7,,,,,,68.4,,50.1,,65.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gemma 2 2B,Google,2024-07-31,https://ai.google.dev/gemma/docs/releases,open reports/combined_report.md,Open weights; 2B parameters. Benchmarks from Gemma 2 model card (PT results table).,55.4,ARC-c 25-shot,,,,,52.0,"3-shot, F1",,,,,23.9,"5-shot, maj@1",73.0,10-shot,17.7,pass@1,,,15.0,4-shot,29.6,3-shot,,,,,51.3,"5-shot, top-1",,,,,,,,,,,,,,,,,16.7,5-shot,,,59.4,5-shot,,,70.9,partial score,,,,,,
InternLM2.5 1.8B,Shanghai AI Laboratory,2024-08-05,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 1.8B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternLM2.5 20B,Shanghai AI Laboratory,2024-08-05,https://github.com/InternLM/InternLM,reports/combined_report.md,Open weights; 20B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5 1.5B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 1.5B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,54.7,25-shot,45.1,3-shot,,,,,24.2,5-shot,,,68.5,4-shot,67.9,10-shot,37.2,0-shot,32.9,0-shot,35.0,4-shot,60.2,0-shot,49.6,0-shot,,,60.9,5-shot,28.5,5-shot,58.5,5-shot,54.8,5-shot,47.9,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,37.5,MGSM 8-shot,25.0,Flores-101 5-shot,65.1,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,33.1,0-shot,,,22.1,5-shot,,,46.6,0-shot,65.0,5-shot,,,,,,
Qwen2.5 14B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 14B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,67.3,25-shot,78.2,3-shot,,,,,32.8,5-shot,,,90.2,4-shot,,,56.7,0-shot,51.2,0-shot,55.6,4-shot,76.7,0-shot,63.2,0-shot,,,79.7,5-shot,51.2,5-shot,76.6,5-shot,76.4,5-shot,70.6,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,68.5,MGSM 8-shot,36.2,Flores-101 5-shot,85.9,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,53.5,0-shot,,,43.0,5-shot,,,58.4,0-shot,,,,,,,,
Qwen2.5 32B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 32B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,70.4,25-shot,84.5,3-shot,,,,,48.0,5-shot,,,92.9,4-shot,85.2,10-shot,58.5,0-shot,52.4,0-shot,57.7,4-shot,84.5,0-shot,67.2,0-shot,,,83.3,5-shot,55.1,5-shot,82.0,5-shot,80.9,5-shot,75.4,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,73.7,MGSM 8-shot,37.3,Flores-101 5-shot,88.4,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,59.4,0-shot,,,44.1,5-shot,,,57.8,0-shot,82.0,5-shot,,,,,,
Qwen2.5 3B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 3B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,56.5,25-shot,56.3,3-shot,,,,,26.3,5-shot,,,79.1,4-shot,74.6,10-shot,42.1,0-shot,36.0,0-shot,42.6,4-shot,57.1,0-shot,49.4,0-shot,,,65.6,5-shot,34.6,5-shot,63.7,5-shot,62.5,5-shot,54.6,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,48.9,MGSM 8-shot,29.3,Flores-101 5-shot,76.6,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,41.2,0-shot,,,27.4,5-shot,,,48.9,0-shot,71.1,5-shot,,,,,,
Qwen2.5 72B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 72B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,72.4,25-shot,86.3,3-shot,,,,,45.9,5-shot,,,91.5,4-shot,87.6,10-shot,59.1,0-shot,51.2,0-shot,62.1,4-shot,84.7,0-shot,69.2,0-shot,,,86.1,5-shot,58.1,5-shot,83.9,5-shot,82.7,5-shot,78.7,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,76.7,MGSM 8-shot,39.0,Flores-101 5-shot,89.6,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,60.5,0-shot,,,42.4,5-shot,,,60.4,0-shot,83.9,5-shot,,,,,,
Qwen2.5 7B,Alibaba,2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; 7B parameters. Benchmarks from Qwen2.5 base model evaluation (settings per release blog).,63.7,25-shot,70.4,3-shot,,,,,36.4,5-shot,,,85.4,4-shot,80.2,10-shot,57.9,0-shot,50.6,0-shot,49.8,4-shot,74.9,0-shot,62.9,0-shot,,,74.2,5-shot,45.0,5-shot,71.1,5-shot,72.3,5-shot,59.4,M3Exam 5-shot; IndoMMLU 3-shot; ruMMLU 5-shot; mMMLU 5-shot,57.8,MGSM 8-shot,32.4,Flores-101 5-shot,79.3,BELEBELE 5-shot; XCOPA 5-shot; XWinograd 5-shot; XStoryCloze 0-shot; PAWS-X 5-shot,50.3,0-shot,,,36.0,5-shot,,,56.4,0-shot,75.9,5-shot,,,,,,
Qwen2.5-1.5B,Alibaba (Qwen Team),2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2.5 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5-14B,Alibaba (Qwen Team),2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2.5 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5-32B,Alibaba (Qwen Team),2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2.5 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5-72B,Alibaba (Qwen Team),2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; non-Apache license per Qwen2.5 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5-7B,Alibaba (Qwen Team),2024-09-19,https://qwenlm.github.io/blog/qwen2.5/,open reports/combined_report.md,Open weights; Apache 2.0 license per Qwen2.5 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.2 11B,Meta,2024-09-25,https://about.fb.com/ltam/news/2024/09/llama-3-2-revolucionando-la-ia-y-la-vision-de-vanguardia-con-modelos-abiertos-y-personalizables/,open reports/combined_report.md,Open weights; 11B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.2 1B,Meta,2024-09-25,https://about.fb.com/ltam/news/2024/09/llama-3-2-revolucionando-la-ia-y-la-vision-de-vanguardia-con-modelos-abiertos-y-personalizables/,open reports/combined_report.md,Open weights; 1B parameters. Benchmarks from Meta Llama 3.2 model card (base pretrained models table).,32.8,25-shot (acc_char),,,,,28.0,3-shot (F1),,,,,,,,,,,,,,,,,,,,,32.2,5-shot (macro_avg/acc_char),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.2 3B,Meta,2024-09-25,https://about.fb.com/ltam/news/2024/09/llama-3-2-revolucionando-la-ia-y-la-vision-de-vanguardia-con-modelos-abiertos-y-personalizables/,open reports/combined_report.md,Open weights; 3B parameters. Benchmarks from Meta Llama 3.2 model card (base pretrained models table).,69.1,25-shot (acc_char),,,,,45.2,3-shot (F1),,,,,,,,,,,,,,,,,,,,,58.0,5-shot (macro_avg/acc_char),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.2 90B,Meta,2024-09-25,https://about.fb.com/ltam/news/2024/09/llama-3-2-revolucionando-la-ia-y-la-vision-de-vanguardia-con-modelos-abiertos-y-personalizables/,open reports/combined_report.md,Open weights; 90B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLMo 2 13B,Ai2,2024-11-26,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 13B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLMo 2 7B,Ai2,2024-11-26,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.3 70B,Meta,2024-12-06,https://techcrunch.com/2024/12/06/meta-unveils-a-new-more-efficient-llama-model/,open reports/combined_report.md,Open weights; 70B parameters. Benchmarks from Meta Llama 3.3 70B Instruct model card (instruction-tuned results).,,,,,,,,,,,50.5,0-shot CoT,,,,,88.4,0-shot pass@1,,,77.0,0-shot CoT (sympy_intersection_score),87.6,0-shot pass@1 (EvalPlus base),,,91.1,0-shot (em),86.0,0-shot CoT (macro_avg/acc),68.9,5-shot CoT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,92.1,reported (instruction following)
Mistral Small 3 24B,Mistral AI,2025-01-30,https://mistral.ai/fr/news/mistral-small-3,open reports/combined_report.md,Open weights; 24B parameters. Benchmarks from Mistral-Small-24B-Instruct-2501 model card (publicly accessible benchmarks); scores reported as fractions (0-1).,,,,,,,,,0.453,"5-shot CoT (main, instruct)",,,0.706,math_instruct (per model card),,,0.848,instruct pass@1,,,,,,,,,,,,,0.663,5-shot CoT (instruct),,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.829,reported (ifeval)
OLMo 2 32B,Ai2,2025-03-13,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 32B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen2.5-Omni-7B,Alibaba,2025-03-27,https://qwenlm.github.io/blog/qwen2.5-omni/,open reports/combined_report.md,Open weights; 7B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qwen3 1.7B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights (Apache 2.0 for dense models per release blog). Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,54.8,3-shot CoT,,,,,16.6,5-shot CoT,,,67.7,8-shot CoT,,,,,,,30.5,4-shot CoT,59.3,3-shot CoT,,,52.2,8-shot CoT,52.1,5-shot CoT,35.4,5-shot CoT,,,,,,,,,,,,,11.8,,,,,,,,,,,,,,,,,
Qwen3 14B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights (Apache 2.0 for dense models per release blog). Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,75.6,3-shot CoT,,,,,36.7,5-shot CoT,,,86.8,8-shot CoT,,,,,,,64.8,4-shot CoT,82.9,3-shot CoT,,,76.2,8-shot CoT,69.9,5-shot CoT,49.1,5-shot CoT,,,,,,,,,,,,,32.5,,,,,,,,,,,,,,,,,
Qwen3 30B-A3B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights; Qwen3 30B-A3B is a MoE model open-weighted per release blog. Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,78.7,3-shot CoT,,,,,43.4,5-shot CoT,,,86.7,8-shot CoT,,,,,,,70.9,4-shot CoT,85.8,3-shot CoT,,,80.5,8-shot CoT,71.1,5-shot CoT,50.3,5-shot CoT,,,,,,,,,,,,,43.0,,,,,,,,,,,,,,,,,
Qwen3 32B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights (Apache 2.0 for dense models per release blog). Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,81.5,3-shot CoT,,,,,45.8,5-shot CoT,,,88.2,8-shot CoT,,,,,,,77.3,4-shot CoT,86.8,3-shot CoT,,,82.8,8-shot CoT,75.4,5-shot CoT,54.0,5-shot CoT,,,,,,,,,,,,,35.8,,,,,,,,,,,,,,,,,
Qwen3 4B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights (Apache 2.0 for dense models per release blog). Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,63.9,3-shot CoT,,,,,26.1,5-shot CoT,,,80.3,8-shot CoT,,,,,,,43.3,4-shot CoT,71.5,3-shot CoT,,,65.8,8-shot CoT,60.5,5-shot CoT,42.0,5-shot CoT,,,,,,,,,,,,,20.7,,,,,,,,,,,,,,,,,
Qwen3 8B,Alibaba,2025-04-29,https://qwenlm.github.io/blog/qwen3/,open reports/combined_report.md,Open weights (Apache 2.0 for dense models per release blog). Benchmarks from Qwen3 technical report (arXiv 2505.09388).,,,70.1,3-shot CoT,,,,,31.6,5-shot CoT,,,84.6,8-shot CoT,,,,,,,55.6,4-shot CoT,79.1,3-shot CoT,,,72.9,8-shot CoT,66.4,5-shot CoT,46.0,5-shot CoT,,,,,,,,,,,,,28.1,,,,,,,,,,,,,,,,,
OLMo 2 1B,Ai2,2025-05-01,https://allenai-web-sandbox.allen.ai/olmo/release-notes,open reports/combined_report.md,Open weights; 1B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caller 32B,Arcee AI,2025-06-30,https://www.arcee.ai/blog/arcee-ai-launches-open-weights-initiative,open reports/combined_report.md,Open weights; 32B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLM-4-32B-Base-32K,Arcee AI,2025-06-30,https://www.arcee.ai/blog/arcee-ai-launches-open-weights-initiative,open reports/combined_report.md,Open weights; 32B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Homunculus 12B,Arcee AI,2025-06-30,https://www.arcee.ai/blog/arcee-ai-launches-open-weights-initiative,open reports/combined_report.md,Open weights; 12B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperNova 70B,Arcee AI,2025-06-30,https://www.arcee.ai/blog/arcee-ai-launches-open-weights-initiative,open reports/combined_report.md,Open weights; 70B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Virtuoso-Large 72B,Arcee AI,2025-06-30,https://www.arcee.ai/blog/arcee-ai-launches-open-weights-initiative,open reports/combined_report.md,Open weights; 72B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gpt-oss-120B,OpenAI,2025-08-05,https://openai.com/index/introducing-gpt-oss/,open reports/combined_report.md,Open weights; 120B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gpt-oss-20B,OpenAI,2025-08-05,https://openai.com/index/introducing-gpt-oss/,open reports/combined_report.md,Open weights; 20B parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apertus 70B,Swiss AI,2025-09-02,https://ethz.ch/en/news-and-events/eth-news/news/2025/09/eth-zurich-and-epfl-to-launch-switzerland-s-first-fully-open-large-language-model-apertus.html,open reports/combined_report.md,Open weights; 70B parameters. Benchmarks from Apertus model card (pretraining eval).,70.6,ARC (as reported; pretraining eval),,,,,,,,,,,,,64.0,as reported; pretraining eval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73.3,as reported; pretraining eval,,,,,,
Apertus 8B,Swiss AI,2025-09-02,https://ethz.ch/en/news-and-events/eth-news/news/2025/09/eth-zurich-and-epfl-to-launch-switzerland-s-first-fully-open-large-language-model-apertus.html,open reports/combined_report.md,Open weights; 8B parameters. Benchmarks from Apertus model card (pretraining eval).,72.7,ARC (as reported; pretraining eval),,,,,,,,,,,,,59.8,as reported; pretraining eval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,70.6,as reported; pretraining eval,,,,,,
RecurrentGemma 2B,Google,2024-06-11,https://ai.google.dev/gemma/docs/recurrentgemma/model_card,,Open weights; 2B parameters. Benchmarks from RecurrentGemma model card (base results table).,42.3,ARC-c,,,,,,,,,,,13.4,maj@1,71.0,0-shot,21.3,pass@1,,,11.0,4-shot,28.8,3-shot,,,,,38.4,"5-shot, top-1",,,,,,,,,,,,,,,,,11.5,5-shot,,,52.5,5-shot,,,67.8,partial score,,,,,,
Qwen3-Next-80B-A3B-Instruct,Alibaba,2025-10-14,https://www.alibabacloud.com/blog/qwen3-next-towards-ultimate-training-%26-inference-efficiency_603319,open reports/combined_report.md,Open weights; 80B total parameters (A3B); instruction-tuned. Benchmarks from Qwen3-Next model card.,,,,,,,,,72.9,as reported (model card),,,,,,,,,,,,,,,,,,,,,80.6,as reported (model card),90.9,as reported (model card),,,,,,,,,,,87.8,as reported (model card),,,,,,,,,,,,,,,87.6,as reported (model card)
Qwen3-Next-80B-A3B-Thinking,Alibaba,2025-10-14,https://www.alibabacloud.com/blog/qwen3-next-towards-ultimate-training-%26-inference-efficiency_603319,open reports/combined_report.md,Open weights; 80B total parameters (A3B); thinking model. Benchmarks from Qwen3-Next model card.,,,,,,,,,77.2,as reported (model card),,,,,,,,,,,,,,,,,,,,,82.7,as reported (model card),92.5,as reported (model card),,,,,,,,,,,,,,,,,,,,,,,,,,,88.9,as reported (model card)
OLMo 3 7B Instruct,AI2,2025-11-20,https://allenai.org/blog/olmo-3,open reports/combined_report.md,Open weights; 7B parameters; instruction-tuned. Benchmarks from OLMo 3 model card.,,,71.2,as reported (model card),,,,,40.4,as reported (model card),,,,,,,,,77.2,as reported (model card),87.3,as reported (model card),,,60.2,as reported (model card),,,69.1,as reported (model card),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,85.6,as reported (model card)
OLMo 3 32B Think,AI2,2025-11-20,https://allenai.org/blog/olmo-3,open reports/combined_report.md,Open weights; 32B parameters; thinking model. Benchmarks from OLMo 3 model card.,,,89.8,as reported (model card),,,,,58.1,as reported (model card),,,,,,,,,91.4,as reported (model card),96.1,as reported (model card),,,68.0,as reported (model card),,,85.4,as reported (model card),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,89.0,as reported (model card)
OLMo 3.1 32B Think,AI2,2025-12-12,https://allenai.org/blog/olmo-3,open reports/combined_report.md,Open weights; 32B parameters; thinking model. Benchmarks from OLMo 3.1 model card.,,,88.6,as reported (model card),,,,,57.5,as reported (model card),,,,,,,,,91.5,as reported (model card),96.2,as reported (model card),,,68.3,as reported (model card),,,86.4,as reported (model card),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,93.8,as reported (model card)
NVIDIA Nemotron-3 Nano 30B-A3B,NVIDIA,2025-12-15,https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,open reports/combined_report.md,Open weights; 30B total parameters (A3B). Benchmarks from NVIDIA Nemotron 3 model card (no tools).,,,,,,,,,73.0,no tools (model card),,,,,,,,,,,,,,,,,,,,,78.3,no tools (model card),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Granite 4.0 Micro 3B Instruct,IBM,2025-10-02,https://huggingface.co/ibm-granite/granite-4.0-micro,open reports/combined_report.md,Open weights (Apache 2.0); 3B parameters; instruction-tuned. Benchmarks from Granite 4.0 model card (evaluation results table).,,,72.48,3-shot CoT,,,,,30.14,0-shot CoT,,,85.45,8-shot,,,80,pass@1,72.0,pass@1,,,72,pass@1,64.0,pass@1,28.56,8-shot,65.98,5-shot,44.5,5-shot CoT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,82.31,average (Instruct/Prompt strict)
Granite 4.0 H Micro 3B Instruct,IBM,2025-10-02,https://huggingface.co/ibm-granite/granite-4.0-h-micro,open reports/combined_report.md,Open weights (Apache 2.0); 3B parameters; instruction-tuned. Benchmarks from Granite 4.0 model card (evaluation results table).,,,69.36,3-shot CoT,,,,,32.15,0-shot CoT,,,81.35,8-shot,,,81,pass@1,75.0,pass@1,,,73,pass@1,64.0,pass@1,44.48,8-shot,67.43,5-shot,43.48,5-shot CoT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,84.32,average (Instruct/Prompt strict)
Granite 4.0 H Tiny 7B Instruct,IBM,2025-10-02,https://huggingface.co/ibm-granite/granite-4.0-h-tiny,open reports/combined_report.md,Open weights (Apache 2.0); 7B parameters; instruction-tuned. Benchmarks from Granite 4.0 model card (evaluation results table).,,,66.34,3-shot CoT,,,,,32.59,0-shot CoT,,,84.69,8-shot,,,83,pass@1,76.0,pass@1,,,80,pass@1,69.0,pass@1,45.36,8-shot,68.65,5-shot,44.94,5-shot CoT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,81.44,average (Instruct/Prompt strict)
Granite 4.0 H Small 32B Instruct,IBM,2025-10-02,https://huggingface.co/ibm-granite/granite-4.0-h-small,open reports/combined_report.md,Open weights (Apache 2.0); 32B parameters; instruction-tuned. Benchmarks from Granite 4.0 model card (evaluation results table).,,,81.62,3-shot CoT,,,,,40.63,0-shot CoT,,,87.27,8-shot,,,88,pass@1,83.0,pass@1,,,84,pass@1,71.0,pass@1,38.72,8-shot,78.44,5-shot,55.47,5-shot CoT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,87.55,average (Instruct/Prompt strict)
Llama 3 8B Instruct,Meta,2024-04-18,https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct,open reports/combined_report.md,Open weights; 8B parameters; instruction-tuned. Benchmarks from Llama 3 8B Instruct model card (instruction-tuned table includes 70B).,,,,,,,,,34.2,0-shot,,,79.6,8-shot CoT,,,62.2,0-shot,,,30.0,4-shot CoT,,,,,,,68.4,5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3 70B Instruct,Meta,2024-04-18,https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct,open reports/combined_report.md,Open weights; 70B parameters; instruction-tuned. Benchmarks from Llama 3 8B Instruct model card (instruction-tuned table includes 70B).,,,,,,,,,39.5,0-shot,,,93.0,8-shot CoT,,,81.7,0-shot,,,50.4,4-shot CoT,,,,,,,82.0,5-shot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.1 8B Instruct,Meta,2024-07-24,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,open reports/combined_report.md,Open weights; 8B parameters; instruction-tuned. Benchmarks from Llama 3.1 8B Instruct model card (instruction-tuned table includes 70B).,83.4,ARC-C 0-shot,,,,,,,30.4,0-shot,,,84.5,8-shot CoT (em_maj1@1),,,72.6,0-shot pass@1,,,51.9,CoT 0-shot (final_em),,,72.8,MBPP++ base version 0-shot pass@1,68.9,CoT 0-shot em,69.4,5-shot,48.3,5-shot CoT,,,,,,,,,,,,,50.8,MultiPL-E HumanEval 0-shot pass@1,,,,,,,,,,,,,,,80.4,as reported
Llama 3.1 70B Instruct,Meta,2024-07-24,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,open reports/combined_report.md,Open weights; 70B parameters; instruction-tuned. Benchmarks from Llama 3.1 8B Instruct model card (instruction-tuned table includes 70B).,94.8,ARC-C 0-shot,,,,,,,46.7,0-shot,,,95.1,8-shot CoT (em_maj1@1),,,80.5,0-shot pass@1,,,68.0,CoT 0-shot (final_em),,,86.0,MBPP++ base version 0-shot pass@1,86.9,CoT 0-shot em,83.6,5-shot,66.4,5-shot CoT,,,,,,,,,,,,,65.5,MultiPL-E HumanEval 0-shot pass@1,,,,,,,,,,,,,,,87.5,as reported
Llama 3.2 1B Instruct,Meta,2024-09-25,https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct,open reports/combined_report.md,Open weights; 1B parameters; instruction-tuned. Benchmarks from Llama 3.2 1B Instruct model card (instruction-tuned MMLU table includes 3B).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,49.3,5-shot (bf16),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Llama 3.2 3B Instruct,Meta,2024-09-25,https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct,open reports/combined_report.md,Open weights; 3B parameters; instruction-tuned. Benchmarks from Llama 3.2 1B Instruct model card (instruction-tuned MMLU table includes 3B).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,63.4,5-shot (bf16),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Granite 3.1 1B-A400M Instruct,IBM,2024-12-18,https://huggingface.co/ibm-granite/granite-3.1-2b-instruct,open reports/combined_report.md,Open weights (Apache 2.0); 1B parameters; instruction-tuned. Benchmarks from Granite 3.1 2B Instruct model card (Open LLM Leaderboard v1/v2 tables include multiple sizes).,42.66,Open LLM Leaderboard v1,6.18,Open LLM Leaderboard v2,,,,,0.0,Open LLM Leaderboard v2,,,33.88,Open LLM Leaderboard v1,65.97,Open LLM Leaderboard v1,,,,,4.08,Open LLM Leaderboard v2 (MATH Lvl 5),,,,,,,26.13,Open LLM Leaderboard v1,2.41,Open LLM Leaderboard v2,,,,,,,,,,,,,,,,,,,,,46.77,Open LLM Leaderboard v1,62.35,Open LLM Leaderboard v1,,,,,46.86,Open LLM Leaderboard v2
Granite 3.1 2B Instruct,IBM,2024-12-18,https://huggingface.co/ibm-granite/granite-3.1-2b-instruct,open reports/combined_report.md,Open weights (Apache 2.0); 2B parameters; instruction-tuned. Benchmarks from Granite 3.1 2B Instruct model card (Open LLM Leaderboard v1/v2 tables include multiple sizes).,54.61,Open LLM Leaderboard v1,21.82,Open LLM Leaderboard v2,,,,,5.26,Open LLM Leaderboard v2,,,52.76,Open LLM Leaderboard v1,75.14,Open LLM Leaderboard v1,,,,,11.33,Open LLM Leaderboard v2 (MATH Lvl 5),,,,,,,55.31,Open LLM Leaderboard v1,20.21,Open LLM Leaderboard v2,,,,,,,,,,,,,,,,,,,,,59.42,Open LLM Leaderboard v1,67.48,Open LLM Leaderboard v1,,,,,62.86,Open LLM Leaderboard v2
Granite 3.1 3B-A800M Instruct,IBM,2024-12-18,https://huggingface.co/ibm-granite/granite-3.1-2b-instruct,open reports/combined_report.md,Open weights (Apache 2.0); 3B parameters; instruction-tuned. Benchmarks from Granite 3.1 2B Instruct model card (Open LLM Leaderboard v1/v2 tables include multiple sizes).,50.42,Open LLM Leaderboard v1,16.69,Open LLM Leaderboard v2,,,,,5.15,Open LLM Leaderboard v2,,,48.97,Open LLM Leaderboard v1,73.01,Open LLM Leaderboard v1,,,,,10.35,Open LLM Leaderboard v2 (MATH Lvl 5),,,,,,,52.19,Open LLM Leaderboard v1,12.75,Open LLM Leaderboard v2,,,,,,,,,,,,,,,,,,,,,49.71,Open LLM Leaderboard v1,64.87,Open LLM Leaderboard v1,,,,,55.16,Open LLM Leaderboard v2
Granite 3.1 8B Instruct,IBM,2024-12-18,https://huggingface.co/ibm-granite/granite-3.1-2b-instruct,open reports/combined_report.md,Open weights (Apache 2.0); 8B parameters; instruction-tuned. Benchmarks from Granite 3.1 2B Instruct model card (Open LLM Leaderboard v1/v2 tables include multiple sizes).,62.62,Open LLM Leaderboard v1,34.09,Open LLM Leaderboard v2,,,,,8.28,Open LLM Leaderboard v2,,,73.84,Open LLM Leaderboard v1,84.48,Open LLM Leaderboard v1,,,,,21.68,Open LLM Leaderboard v2 (MATH Lvl 5),,,,,,,65.34,Open LLM Leaderboard v1,28.19,Open LLM Leaderboard v2,,,,,,,,,,,,,,,,,,,,,66.23,Open LLM Leaderboard v1,75.37,Open LLM Leaderboard v1,,,,,72.08,Open LLM Leaderboard v2
